# -*- coding: utf-8 -*-
"""antitrust_scape.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lngI8pkFzM8ktpjcLnREGWP6K4BEbSfE

The recent paper from Besley, Fontana and Limodio (2020, AEJ Insights) uses a database that is avaiable digitally but not in a format able to be read in actively as a .do file. I scrape Hylton and Deng's Antitrust Database-- which is used in the paper, with this code. Note. Various measures of anti-trust that go into the rankings/scores are not scraped. This must be worked on.
"""

!pip3 install country_converter

import requests
from bs4 import BeautifulSoup as bs
from urllib.parse import urlparse, urljoin
import urllib
import  nltk
import webbrowser
import geograpy
import pycountry
import pandas as pd
from urllib.request import urlopen
from geograpy import places
from geograpy import extraction
import scrapy
import re
from dateutil.parser import parse
import calendar
from string import digits
from collections import OrderedDict
import datetime
import country_converter as coco

res = requests.get('http://antitrustworldwiki.com/antitrustwiki/index.php/Main_Page')
soup = bs(res.text, "html.parser")
wiki_dict = {}
linklist = []
for link in soup.find_all("a"):
    url = link.get("href", "")
    if "antitrustwiki/index.php/" in url:
        wiki_dict[link.text.strip()] = url
        linklist.append(wiki_dict)
countrylist = pd.DataFrame.from_dict(linklist)
#pycountries do not recognize these irregular naming conventions
columns= list(country.name for country in pycountry.countries)
exceptions = ["Bolivia", "Venezuela", "Cote d'Ivoire", 'Vietnam', 'Iran',
              'South Korea','Sudan','Macau','Republic of Ireland', 
              'Slovak Republic', 'Bosnia-Herzegovina', 'Lao PDR', 'Tanzania', 'Taiwan',
              "Macedonia", "Russia", "Moldova", "Faroe Island" "Czech Republic"]
def Union(lst1, lst2): 
    final_list = lst1 + lst2 
    return final_list 
united_columns = Union(columns, exceptions)

countrylist = countrylist[countrylist.columns.intersection(united_columns)]
countrylist= countrylist.drop_duplicates()
country_url =  countrylist.values.tolist()[0] 
string = "http://antitrustworldwiki.com"
list_url = [string+str(x) for x in country_url]

wiki_dict2 = {}
linklist2 = []
for l in list_url:
  r = requests.get(l)
  soup = bs(r.text, "html.parser")
  for link in soup.find_all("a"):
    url2 = link.get("href", "")
    if "antitrustwiki/index.php/" in url2:
        wiki_dict2[link.text.strip()] = url2
        linklist2.append(wiki_dict2)
countrylist2 = pd.DataFrame.from_dict(linklist2)

countrylist2= countrylist2.drop_duplicates()
country_url2 =  countrylist2.values.tolist()[0] 
list_url2 = [string+str(x) for x in country_url2]
linklist3 = []
for x in list_url2:
  for y in list_url:
    if y in x:
      linklist3.append(x)
linklist3 = [x for x in linklist3 if "/EU," not in x]

year=[]
appended=[]
dicts={}
for z in linklist3:
  req = requests.get(z)
  soup3 = bs(req.text, 'html.parser')
  for nat in soup3.find_all('h1'):
    for line in soup3.find_all('b'):
      if not "Score" in line.text:continue  
      appended.append([nat.get_text(strip=True), line.get_text(strip=True)])

appended

apd = pd.DataFrame.from_dict(appended)
apd[1] = apd1[1].map(lambda x: x.replace('=','').replace('Score','').strip(' '))
months = []
for i in range(1,12):
  months.append(calendar.month_name[i])
for x in months:
  apd[0] = apd[0].str.replace(x, "")
apd[0] = apd[0].str.replace(",", "")
apd[0] = apd[0].str.replace(")", "")
apd[0] = apd[0].str.replace("(", "")
apd

apd[3] = apd[0].str.extract("(\d\d\d\d)")
apd[0] = apd[0].str.replace('\d+', '')

"""There is certainly a better way to do this... Code to be optimized"""

apd= apd.rename(columns={1: "Score", 0: "Country", 3: "Year" })
apd["Country"] = apd["Country"].str.replace("December", "")
apd["Country"] = apd["Country"].str.replace("February", "")
apd["Country"] = apd["Country"].str.replace("January", "")
apd["Country"] = apd["Country"].str.replace("March", "")
apd["Country"] = apd["Country"].str.replace("April", "")
apd["Country"] = apd["Country"].str.replace("May", "")
apd["Country"] = apd["Country"].str.replace("June", "")
apd["Country"] = apd["Country"].str.replace("July", "")
apd["Country"] = apd["Country"].str.replace("August", "")
apd["Country"] = apd["Country"].str.replace("September", "")
apd["Country"] = apd["Country"].str.replace("October", "")
apd["Country"] = apd["Country"].str.replace("November", "")
apd["Country"] = apd["Country"].str.replace("Hungary Febuary", "Hungary")
apd["Country"] = apd["Country"].str.replace("Jan", "")
apd["Country"] = apd["Country"].str.replace("Serbia-Montenegro", "Serbia")
apd["Country"] = apd["Country"].str.replace(".", "")
apd["Country"] = apd["Country"].str.replace("Bosnia-Herzegovina", "Bosnia and Herzegovina")
apd["Country"] = apd["Country"].str.replace("Lao PDF", "Laos")
apd["Country"] = apd["Country"].str.replace("South Korea", "Korea, Republic of")
apd["Country"] = apd["Country"].str.replace("Aug", "")
apd

def do_fuzzy_search(country):
    try:
        result = pycountry.countries.search_fuzzy(country)
    except:
        return print('An exception flew by!')
        result = "LAO"
        raise
    else:
        return result[0].alpha_3
apd["Country"] = apd["Country"].apply(lambda country: do_fuzzy_search(country))

apd.to_excel('antitrust.xlsx', index=False, encoding='utf-8')
apd

from google.colab import files
files.download('antitrust.xlsx')

